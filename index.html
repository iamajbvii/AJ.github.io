<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>AJ's Website</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;800&display=swap" rel="stylesheet">
  <style>
    body {
      margin: 0;
      padding: 0;
      background: linear-gradient(to right, #eef2f3, #dfe9f3);
      font-family: 'Inter', sans-serif;
      color: #2d3436;
    }

    .container {
      max-width: 900px;
      margin: 60px auto;
      padding: 40px;
      background: #ffffff;
      border-radius: 16px;
      box-shadow: 0 12px 30px rgba(0, 0, 0, 0.08);
    }

    .header {
      display: flex;
      align-items: center;
      gap: 20px;
      margin-bottom: 30px;
      flex-wrap: wrap;
      justify-content: center;
    }

    .profile-pic {
      width: 70px;
      height: 70px;
      border-radius: 50%;
      object-fit: cover;
      border: 3px solid #1a73e8;
    }

    .title-text {
      font-size: 2.2rem;
      font-weight: 800;
      color: #1a73e8;
      text-align: center;
    }

    h2 {
      font-size: 1.6rem;
      color: #34495e;
      margin-top: 20px;
    }

    p {
      font-size: 1.05rem;
      line-height: 1.8;
      margin-top: 10px;
    }

    .disco-list {
      list-style: none;
      padding-left: 0;
    }

    .disco-list li {
      margin: 8px 0;
    }

    .disco-list a {
      text-decoration: none;
      font-weight: bold;
      font-size: 1.1rem;
      animation: rainbow 2s infinite linear, glow 1s infinite alternate;
      display: inline-block;
      transition: transform 0.3s ease;
    }

    .disco-list a:hover {
      transform: scale(1.15) rotate(3deg);
    }

    @keyframes rainbow {
      0% { color: red; }
      16% { color: orange; }
      32% { color: yellow; }
      48% { color: green; }
      64% { color: blue; }
      80% { color: indigo; }
      100% { color: violet; }
    }

    @keyframes glow {
      0% {
        text-shadow: 0 0 5px #fff, 0 0 10px #ff0, 0 0 20px #f0f, 0 0 40px #0ff;
      }
      100% {
        text-shadow: 0 0 10px #fff, 0 0 20px #0ff, 0 0 30px #f0f, 0 0 60px #ff0;
      }
    }

    @media (max-width: 600px) {
      .container {
        padding: 25px;
      }

      .title-text {
        font-size: 1.8rem;
      }

      .profile-pic {
        width: 60px;
        height: 60px;
      }
    }
  </style>
</head>
<body>

  <div class="container">
    <div class="header">
      <img src="portrait.png" alt="AJ's photo" class="profile-pic">
      <div class="title-text">Welcome to AJ's Website</div>
    </div>
	
	<h3>Week 4 Report</h2>
	<h3>What I did this week</h3>
	
	<p>After our meeting on Monday, we finally found our concentration. I focused on 
	manually trimming videos that include when a person starts signing and stops signing. 
	I eliminated any footage that involves at least 2 people signing and anything else that 
	is unrelated to signing. I developed a code where I use MediaPipe Holistic Tracking. It 
	includes 468 points on your body, head, arms, and hands. We narrowed it to 85 points. 
	21 points on both hands (42), 6 points on the body, and 37 points on the face. They 
	combined for 85 points. With the edited video, I ran the video with a code that will 
	track the 85 points and log every time any point disappears from tracking. When the point 
	disappears, the code will check the timestamp of when it disappeared and the location 
	((X, Y) coordinates) and log this information. We plan to measure whether Deaf Mosaic 
	is as good as newer videos from the University Communication. We plan to observe the data
	 when the tracking disappears and figure out which videos we should preserve for data and 
	 which should be eliminated. We changed our approach as Mohammed will log all the timestamps 
	 on when a person starts signing and stops. Instead of manually trimming the videos myself. 
	 I developed a code that will look at the start time stamp and end time stamp and extract 
	 the video within that time stamp. The code was successful as it extracted videos at every 
	 time stamp and created the video in separate MP4 files. This approach saves a lot of time on my end.</p>

	<h3>What I plan to do next week</h3>
	<p>I need to complete trimming all the videos that I received from Mohammed. After this, 
	I need to go through every trimmed video with my code that will track all 85 points and 
	log whether the points disappear. Then, I will develop a database where I can measure 
	and observe which videos have the best data.</p>

	
	
	<h2>Week 3 Report</h2>
	<h3>What I did this week</h3>

	<p>The instructions were not clear on what we were supposed to research. 
	After having a meeting with Christian Vogler to clarify what we are focusing on. 
	The end goal became clearer to us. However, we had the meeting two days before we were 
	supposed to present our presentation. With that short deadline, we tried our best to gather 
	the information that we wanted to use in our presentation. We found sufficient information, 
	but I know that we could have done better if we had 3 weeks to find information and prepare 
	for the presentation.
	After the meeting, we finally understood what we were supposed to do. I believe that we will 
	be able to move forward from our literature review and start implementing what we have learned 
	into practice. I plan to develop or find a dataset that consists of all the alphabets in ASL. 
	I aim to find a 3-D model for every handshape so the training model can match the 3-D model of 
	the handshape to analyze the video like Deaf Mosaics. I want my model to be able to identify what 
	handshape the person is saying when they fingerspell during the video</p> 


    <h2>Week 2 Report</h2>
    <h3>ASL data</h3>
    <ul>
      <li><a href="https://www.youtube.com/@SorensonVRSVideos" target="_blank">Sorenson - YouTube</a></li>
      <li><a href="https://www.youtube.com/@TheDailyMoth" target="_blank">Daily Moth - YouTube</a></li>
      <li><a href="https://www.dailymoth.com/blog" target="_blank">Daily Moth - Website</a></li>
      <li><a href="https://members.dailymoth.com" target="_blank">Daily Moth - (requires membership)</a></li>
      <li><a href="https://www.youtube.com/@GallaudetU" target="_blank">Gallaudet University (GU) - YouTube</a></li>
      <li><a href="https://www.youtube.com/@GallaudetUniversityPress" target="_blank">GU Press - YouTube</a></li>
      <li><a href="https://www.youtube.com/@RITLibraries/videos" target="_blank">Rochester Institute of Technology (RIT) - YouTube</a></li>
      <li><a href="https://www.youtube.com/@ConvoCommunications" target="_blank">Convo - YouTube</a></li>
    </ul>

    <h3>What I did this week</h3>
    <p>
      I developed a code that will start clipping video when it detects that hands are moving, 
      and will stop clipping when the hands stop moving and are at rest. I tested the code with 
      one of my final project presentations, and it seems to work. It did successfully clip video 
      when I started talking, and stopped clipping when I stopped talking. Although the code is not 
      perfect, as it clips when I do gestures or gesticulations. This led me to think of my research question.
      I created a draft PowerPoint for presentation next Friday and started a draft on the Lit Review Paper.
    </p>

    <h3>Research Question</h3>
    <p>
      How can AI determine the difference between when a person is talking with sign language 
      and when a person is talking using gesture and/or gesticulation?<br>
      Is it possible to develop a model that can naturally detect sign language and 
      normal gestures and tell the difference between them?<br>
      What are the qualifications or requirements to determine if the video is qualified 
      to be used in training data to train the model to detect the difference between sign 
      language and gesture/gesticulation?
    </p>

    <h3>Action Plan</h3>
    <p>
      <ul>
	      <li>Need to find which videos are copyrighted and which aren’t.</li>
	      <li>Contact them to obtain permission.</li>
	      <li>I did develop a draft email, but I haven’t sent the email.</li>
	      <li>Get access to the NAS so I can start testing video in my code.</li>
	      <li>Start gathering informations from articles for Lit Review Paper</li>
      </ul>
    </p>

    <h2>Week 1 Report</h2>
    <p>
      My first week at REU AICT research was mainly focused on finding research papers for the literature review. 
      My team includes Abdullah Alghamdi and me. We focus on finding ASL data and determining how the research 
      papers touch topics such as methods to track signs and body, the amount of time the face and hands are 
      detected (signing and not signing), criteria for estimating the body pose, use open or closed captioning, 
      and the percentage of success. I have developed two different methods for note-taking. One method involves 
      a detailed explanation of whether the research papers talk about each topic. Other methods include simplified 
      checklists and brief notes. This method is known as Too Long; Didn’t Read (TL;DR). I found 8 research papers 
      that I can use. I went through their topics and created notes on whether they satisfy the checklist or not, 
      and put them all in two documents.
    </p>
  </div>

</body>
</html>
